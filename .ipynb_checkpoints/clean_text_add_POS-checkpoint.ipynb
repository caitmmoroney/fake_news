{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text & Add Column of POS Tags\n",
    "\n",
    "In order to obtain features from our articles dataset, we will need to vectorize the text; however, before we can do that we should first clean the text to remove punctuation, convert the text to lowercase, expand contractions, and lemmatize words. We will also use POS tag counts as features in our predictive modeling, so we will add a column that includes text where the words have been replaced with their POS tags. This column can then be run through sklearn's CountVectorizer to obtain POS tag counts efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.help import upenn_tagset\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import other modules..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Gateway Pundit</td>\n",
       "      <td>2018-07-14</td>\n",
       "      <td>REPORT House Conservatives Prepare to Impeach ...</td>\n",
       "      <td>House GOP lawmakers are preparing to push to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>oann</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>French policeman who took place of hostage die...</td>\n",
       "      <td>PARIS (Reuters)  A gendarme who was shot three...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New York Daily News</td>\n",
       "      <td>2018-03-24</td>\n",
       "      <td>Attorney for Roy Moore accuser was offered 10G...</td>\n",
       "      <td>An attorney for Leigh Corfman, a woman who acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sputnik</td>\n",
       "      <td>2018-03-23</td>\n",
       "      <td>Martin Vizcarra is New Peruvian President Afte...</td>\n",
       "      <td>Martin Vizcarra is sworn in as Peruvian presid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>oann</td>\n",
       "      <td>2018-04-02</td>\n",
       "      <td>Oil falls 2 percent on Russia output rise pote...</td>\n",
       "      <td>NEW YORK (Reuters)  Oil fell by more than 2 pe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           news_source    pub_date  \\\n",
       "0   The Gateway Pundit  2018-07-14   \n",
       "1                 oann  2018-03-24   \n",
       "2  New York Daily News  2018-03-24   \n",
       "3              Sputnik  2018-03-23   \n",
       "4                 oann  2018-04-02   \n",
       "\n",
       "                                               title  \\\n",
       "0  REPORT House Conservatives Prepare to Impeach ...   \n",
       "1  French policeman who took place of hostage die...   \n",
       "2  Attorney for Roy Moore accuser was offered 10G...   \n",
       "3  Martin Vizcarra is New Peruvian President Afte...   \n",
       "4  Oil falls 2 percent on Russia output rise pote...   \n",
       "\n",
       "                                                text  \n",
       "0  House GOP lawmakers are preparing to push to i...  \n",
       "1  PARIS (Reuters)  A gendarme who was shot three...  \n",
       "2  An attorney for Leigh Corfman, a woman who acc...  \n",
       "3  Martin Vizcarra is sworn in as Peruvian presid...  \n",
       "4  NEW YORK (Reuters)  Oil fell by more than 2 pe...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles = pd.read_csv('./articles_df.csv', index_col = 0)\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove rows with no text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no blank articles in our dataset.\n"
     ]
    }
   ],
   "source": [
    "# get rid of any blank articles\n",
    "if len(articles) == len(articles[articles['text'].notnull()]):\n",
    "    print('There are no blank articles in our dataset.')\n",
    "else:\n",
    "    print(f'There are {k} blank articles in our dataset.')\n",
    "    articles = articles[articles['text'].notnull()].reset_index(drop = True)\n",
    "    print('The blank articles have now been removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no rows with empty strings in our dataset.\n"
     ]
    }
   ],
   "source": [
    "# check for any strings with no word characters\n",
    "no_words = []\n",
    "for i in range(len(articles)):\n",
    "    if re.match('^[\\W_]+$', articles['text'][i]):\n",
    "        no_words.append(i)\n",
    "\n",
    "if len(no_words) == 0:\n",
    "    print('There are no rows with empty strings in our dataset.')\n",
    "else:\n",
    "    print(f'There are {len(no_words)} rows with empty strings in our dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'House GOP lawmakers are preparing to push to impeach Deputy Attorney General Rod Rosenstein as soon as Monday, according to three conservative Capitol Hill sources.\\n\\nFreedom Caucus Chairman Mark Meadows previously drafted the impeachment documents, however nothing has been filed yet.\\n\\nGOP Congressmen are fed up with Rosensteins continued stonewalling of their probe of the FBIs and DOJs corruption, Spygate and Russiagate during the 2016 election.\\n\\nHouse conservatives are preparing a new push to oust Deputy Attorney General Rod Rosenstein, according to three conservative Capitol Hill sources  putting the finishing touches on an impeachment filing even as Rosenstein announced the indictment of 12 Russian intelligence officers for interfering in the 2016 election. House Freedom Caucus Chairman Mark Meadows, in fact, had the impeachment document on the floor of the House at the very moment that Rosenstein spoke to reporters and TV cameras Friday. Conservative sources say they could file the impeachment document as soon as Monday, as Meadows and Freedom Caucus founder Jim Jordan (R-Ohio) look to build Republican support in the House. One source cautioned, however, that the timing was still fluid. It has not been filed today, was all Meadows spokesman Ben Williamson would say. Williamson declined to rule out whether Meadows intended to file the document next week. Republicans could also try to hold Rosenstein in contempt of Congress, if they want to go a step before impeachment.\\n\\nFriday, Rep. Mark Meadows hinted once again Rosenstein attempted to obstruct oversight.\\n\\nFreedom Caucus Chairman Mark Meadows dropped a bombshell Friday afternoon and said it appears the DOJ is continuing their efforts to keep material facts and even witnesses from Congress.\\n\\nMeadows tweeted: Remarkably, we learned new information today suggesting the DOJ had not notified Lisa Page of Congress outstanding interview requests for over 7 months now. The DOJ/FBI appear to be continuing their efforts to keep material facts, and perhaps even witnesses, from Congress.\\n\\nRosenstein was defiant, smug and laughed off lawmakers during a recent Congressional hearing and refused to answer many pertinent Spygate questions.\\n\\nThe Deputy Attorney General has been working overtime to obstruct House conservatives from oversight while running offense for the Deep State with the Mueller witch hunt.\\n\\nBoth Mueller and Rosenstein are out of control and need to be prosecuted and thrown in prison.\\n\\nFridays revelation the DOJ had not notified Lisa Page of Congress outstanding interview requests for over 7 months may be the last straw for GOP lawmakers; its way past time to get rid of Rod Rosenstein.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view example article\n",
    "articles['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Things we might want to remove from the text:\n",
    "# - political rep. tags, e.g. (D-NY) for Democrat, New York\n",
    "# - email addresses asking for tips or comments\n",
    "# - internet domains\n",
    "# - do we want to keep punctuation? (for instance, maybe \"!\" is used more often in fake news articles)\n",
    "#\n",
    "# Other considerations:\n",
    "# - remove stopwords\n",
    "# - lemmatize words\n",
    "\n",
    "# to convert contractions into full words (before tokenization)\n",
    "contractions = {\n",
    "    \"ain't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"I'd\": \"I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so is\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"'s\": \" is\" # for instances such as \"Caitlin's going to run this code.\"\n",
    "}\n",
    "\n",
    "# to map nltk POS tags to wordnet POS tags\n",
    "tag_dict = {\n",
    "    'J': wordnet.ADJ,\n",
    "    'N': wordnet.NOUN,\n",
    "    'V': wordnet.VERB,\n",
    "    'R': wordnet.ADV\n",
    "}\n",
    "\n",
    "# function to convert nltk_pos tags to wordnet-compatible POS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]\n",
    "\n",
    "# Set list of \"valid\" tags such that when normalizing text, all words tagged with PoS = coordinating conjunction,\n",
    "# cardinal digit, determiner, existential there, preposition/subordinating conjunction, list marker, predeterminer,\n",
    "# possessive ending, personal pronoun, possessive pronoun, to, or interjection are dropped.\n",
    "valid_tags_abbr = 'FJMNRVW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to clean text\n",
    "def clean_str(text, lemmatize = True):\n",
    "    # to drop any internet domains, email addresses, or political rep. tags\n",
    "    text = re.sub(r'(https?://)?\\w+@?\\w+(\\.\\w+)+|\\([DRI]-[A-Z]{2}\\)', '', text)\n",
    "\n",
    "    # iterate over entries in contractions dictionary\n",
    "    for el in list(contractions.keys()):\n",
    "        if el in text.lower(): # check if the ith contraction is in the string\n",
    "            text = re.sub(el, contractions[el], text, flags = re.IGNORECASE) # expand contraction\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    clean_words = []\n",
    "\n",
    "    for word in words:\n",
    "        PoS_tag = pos_tag([word])[0][1]\n",
    "        word = re.sub(r'[_-]', '', word)\n",
    "\n",
    "        # drop words with fewer than 2 characters; drop any punctuation \"words\"; drop words not in\n",
    "        # approved set of PoS tags (defined above)\n",
    "        if (len(word) > 1) and (re.match(r'^\\w+$', word)) and (PoS_tag[0].upper() in valid_tags_abbr):\n",
    "\n",
    "            if lemmatize:\n",
    "                lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                if PoS_tag[0].upper() in 'JNVR':\n",
    "                    word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                else:\n",
    "                    word = lemmatizer.lemmatize(word)\n",
    "\n",
    "            clean_words.append(word)\n",
    "    clean_text = ' '.join(clean_words)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean_str() function to all articles\n",
    "articles['clean_txt'] = articles['text'].map(clean_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no rows where the clean text column is null.\n"
     ]
    }
   ],
   "source": [
    "# check to see if there are any rows where the clean text column is null\n",
    "if len(articles[articles['clean_txt'].notnull()]) == len(articles):\n",
    "    print('There are no rows where the clean text column is null.')\n",
    "else:\n",
    "    k = 0\n",
    "    for i in range(len(articles['clean_txt'])):\n",
    "        if type(articles['clean_txt'][i]) != str:\n",
    "            k += 1\n",
    "    print(f'There are {k} rows where the clean text column is null.')\n",
    "    \n",
    "    # drop articles with null clean text\n",
    "    articles = articles[articles['clean_txt'].notnull()].reset_index(drop = True)\n",
    "    print('These rows have now been removed from the dataframe.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tag text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    }
   ],
   "source": [
    "# all PoS tags\n",
    "upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to replace text with POS tags\n",
    "def str_to_pos(text):\n",
    "    # expand contractions\n",
    "    for el in list(contractions.keys()):\n",
    "        if el.lower() in text.lower():\n",
    "            text = re.sub(el, contractions[el], text, flags = re.IGNORECASE)\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    tags = []\n",
    "    for word in words:\n",
    "        tag = pos_tag([word])[0][1]\n",
    "        tags.append(tag)\n",
    "    pos_text = ' '.join(tags)\n",
    "    \n",
    "    return pos_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NN VBZ PRP$ NN NN . WP VBZ NNS .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test function\n",
    "testText = 'NLP is my favorite class! What\\'s yours?'\n",
    "str_to_pos(testText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 1, 1, 2, 1]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_test = CountVectorizer(lowercase = False, token_pattern='\\w\\w+\\$?')\n",
    "cv_test.fit_transform([str_to_pos(testText)]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NN': 0, 'VBZ': 3, 'PRP$': 2, 'WP': 4, 'NNS': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_test.vocabulary_\n",
    "# NOTE: The punctuation POS tag has been dropped, but this is okay because we will count punctuation use\n",
    "# separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in dataframe with each cell = text replaced with POS tags\n",
    "articles['POS_tags'] = articles['text'].map(str_to_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Updated Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save over existing articles csv file\n",
    "articles.to_csv('articles_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
