{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STAT 696 Final Project - Live Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Here we include code for the in-class demo of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The shipping off of American pork to China dur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The shipping off of American pork to China dur..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load demo articles here, store in pandas dataframe\n",
    "articles_demo = pd.DataFrame({'text': ['The shipping off of American pork to China during a pandemic caused by the Chinese as the UN predicts famines of biblical proportions shouldnâ€™t sit well with any American. Meanwhile, China just reported another case of African swine fever, just one in a dozen cases in the last 2 months, devastating the Chinese herd and increasing demand from U.S. pig farms. And now, the Chinese Coronavirus threatens American food producers. China\\'s Coronavirus is nothing short of an act of war disguised in no-fault propaganda, putting millions out of work and decimating small businesses while bringing our education system to a standstill and undermining our food supply while China openly celebrates the death of American Democracy.']})\n",
    "articles_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to clean text data\n",
    "\n",
    "# to convert contractions picked up by word_tokenize() into full words\n",
    "contractions = {\n",
    "    \"n't\": 'not',\n",
    "    \"'ve\": 'have',\n",
    "    \"'s\": 'is', # note that this will include possessive nouns\n",
    "    'gonna': 'going to',\n",
    "    'gotta': 'got to',\n",
    "    \"'d\": 'would',\n",
    "    \"'ll\": 'will',\n",
    "    \"'re\": 'are',\n",
    "    \"'m\": 'am',\n",
    "    'wanna': 'want to'\n",
    "}\n",
    "\n",
    "# to convert nltk_pos tags to wordnet-compatible PoS tags\n",
    "def convert_pos_wordnet(tag):\n",
    "    tag_abbr = tag[0].upper()\n",
    "    tag_dict = {\n",
    "        'J': wordnet.ADJ,\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV\n",
    "    }\n",
    "                \n",
    "    if tag_abbr in tag_dict:\n",
    "        return tag_dict[tag_abbr]\n",
    "\n",
    "# Set list of \"valid\" tags such that when normalizing text, all words tagged with PoS = coordinating conjunction,\n",
    "# cardinal digit, determiner, existential there, preposition/subordinating conjunction, list marker, predeterminer,\n",
    "# possessive ending, personal pronoun, possessive pronoun, to, or interjection are dropped.\n",
    "valid_tags_abbr = 'FJMNRVW'\n",
    "\n",
    "def clean_text(str_list, lemmatize = True):\n",
    "    clean_list = []\n",
    "    \n",
    "    for text in str_list:\n",
    "        # to drop any internet domains, email addresses, or political rep. \"tags\"\n",
    "        text = re.sub(r'(https?://)?\\w+@?\\w+(\\.\\w+)+|\\([DRI]-[A-Z]{2}\\)', '', text)\n",
    "        words = word_tokenize(text)\n",
    "        clean_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            PoS_tag = pos_tag([word])[0][1]\n",
    "            word = re.sub(r'[_-]', '', word)\n",
    "            \n",
    "            # to change contractions to full word form\n",
    "            if word in contractions:\n",
    "                word = contractions[word]\n",
    "            \n",
    "            # drop words with fewer than 2 characters; drop any punctuation \"words\"; drop words not in\n",
    "            # approved set of PoS tags (defined above)\n",
    "            if (len(word) > 1) and (re.match(r'^\\w+$', word)) and (PoS_tag[0].upper() in valid_tags_abbr):\n",
    "\n",
    "                if lemmatize:\n",
    "                    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    if PoS_tag[0].upper() in 'JNVR':\n",
    "                        word = lemmatizer.lemmatize(word, convert_pos_wordnet(PoS_tag))\n",
    "                    else:\n",
    "                        word = lemmatizer.lemmatize(word)\n",
    "            \n",
    "                clean_words.append(word)\n",
    "        clean_text = ' '.join(clean_words)\n",
    "        clean_list.append(clean_text)\n",
    "    \n",
    "    return clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_demo['clean_txt'] = clean_text(articles_demo['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new column in dataframe with each cell = text replaced with PoS tags\n",
    "pos_column = []\n",
    "for el in articles_demo['text']:\n",
    "    words = word_tokenize(el)\n",
    "    tags = []\n",
    "    for word in words:\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "        tag = pos_tag([word])[0][1]\n",
    "        tags.append(tag)\n",
    "    pos_column.append(' '.join(tags))\n",
    "articles_demo['PoS_tags'] = pos_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hi4_df = pd.read_excel('inquireraugmented.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = hi4_df[hi4_df.Positiv==\"Positiv\"]\n",
    "neg_df = hi4_df[hi4_df.Negativ ==\"Negativ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = pos_df['Entry'].tolist()\n",
    "neg_list = neg_df['Entry'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_word = [ ]\n",
    "for i in range(0, len(pos_list)):\n",
    "    pos_word.append( re.sub(r'[^A-Z]', \"\", pos_list[i]))\n",
    "pos_word=set(pos_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_word = [ ]\n",
    "for i in range(0, len(neg_list)):\n",
    "    neg_word.append( re.sub(r'[^A-Z]', \"\",str(neg_list[i]))) \n",
    "neg_word=set(neg_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scorer(text_input, in_list):\n",
    "    words_set = set(in_list)\n",
    "    text_input = text_input.upper().split(' ')\n",
    "    score = 0\n",
    "    for i in text_input:\n",
    "        if i in words_set:\n",
    "            score += 1\n",
    "    score = score/len(text_input)\n",
    "    return(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_demo['pos_sent']=articles_demo['clean_txt'].apply(lambda x: sentiment_scorer(x, pos_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_demo['neg_sent']=articles_demo['clean_txt'].apply(lambda x: sentiment_scorer(x, neg_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_demo['net_sent'] = articles_demo['pos_sent'] - articles_demo['neg_sent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punctuation Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating CV matrix for punctuation(!?:;)\n",
    "class Punc_Vec:\n",
    "    def __init__(self,\n",
    "                 analyzer = 'char',\n",
    "                 binary = False,\n",
    "                 decode_error = 'strict',\n",
    "                 dtype = np.int64,\n",
    "                 encoding = 'utf-8',\n",
    "                 input = 'content',\n",
    "                 lowercase = False,\n",
    "                 max_df = 1.0,\n",
    "                 min_df = 1,\n",
    "                 ngram_range = (1,1),\n",
    "                 max_features = None,\n",
    "                 strip_accents = None,\n",
    "                 preprocessor = None,\n",
    "                 tokenizer = None,\n",
    "                 stop_words = None,\n",
    "                 vocabulary = None,\n",
    "                 token_pattern = '(?u)\\b\\w\\w+\\b'):\n",
    "        self.analyzer = analyzer\n",
    "        self.binary = binary\n",
    "        self.decode_error = decode_error\n",
    "        self.dtype = dtype\n",
    "        self.encoding = encoding\n",
    "        self.input = input\n",
    "        self.lowercase = lowercase\n",
    "        self.max_df = max_df\n",
    "        self.min_df = min_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.max_features = max_features\n",
    "        self.strip_accents = strip_accents\n",
    "        self.preprocessor = preprocessor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_words = stop_words\n",
    "        self.vocabulary = vocabulary\n",
    "        self.token_pattern = token_pattern\n",
    "        self.c = None\n",
    "        self.v = CountVectorizer(analyzer = self.analyzer,\n",
    "                                 binary = self.binary,\n",
    "                                 decode_error = self.decode_error,\n",
    "                                 dtype = self.dtype,\n",
    "                                 encoding = self.encoding,\n",
    "                                 input = self.input,\n",
    "                                 lowercase = self.lowercase,\n",
    "                                 max_df = self.max_df,\n",
    "                                 min_df = self.min_df,\n",
    "                                 ngram_range = self.ngram_range,\n",
    "                                 max_features = self.max_features,\n",
    "                                 strip_accents = self.strip_accents,\n",
    "                                 preprocessor = self.preprocessor,\n",
    "                                 tokenizer = self.tokenizer,\n",
    "                                 stop_words = self.stop_words,\n",
    "                                 vocabulary = self.vocabulary,\n",
    "                                 token_pattern = self.token_pattern)\n",
    "        self.params = None\n",
    "        self.f = None\n",
    "        self.t = None\n",
    "        self.f_t = None\n",
    "        self.g = None\n",
    "    def get_params(self, deep = True):\n",
    "        self.params = self.v.get_params(deep)\n",
    "        return(self.params)\n",
    "    def fit(self, corpus, y = None):\n",
    "        self.c = corpus.tolist()\n",
    "        punc_only = []\n",
    "        for i in range(len(self.c)):\n",
    "            punc_only.append([])\n",
    "            punc_only[i] = re.sub(r\"[^!?:;]\", \"\", self.c[i])\n",
    "        self.f = self.v.fit(punc_only)\n",
    "    def transform(self, corpus):\n",
    "        self.t = self.v.transform(corpus.tolist())\n",
    "        return(self.t)\n",
    "    def fit_transform(self, corpus, y = None):\n",
    "        self.c = corpus.tolist()\n",
    "        punc_only = []\n",
    "        for i in range(len(self.c)):\n",
    "            punc_only.append([])\n",
    "            punc_only[i] = re.sub(r\"[^!?:;]\", \"\", self.c[i])\n",
    "        self.f_t = self.v.fit_transform(punc_only)\n",
    "        return self.f_t\n",
    "    def get_features(self):\n",
    "        self.g = self.v.get_feature_names()\n",
    "        return(self.g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_txt</th>\n",
       "      <th>PoS_tags</th>\n",
       "      <th>pos_sent</th>\n",
       "      <th>neg_sent</th>\n",
       "      <th>net_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The shipping off of American pork to China dur...</td>\n",
       "      <td>shipping American pork China pandemic cause Ch...</td>\n",
       "      <td>DT NN IN IN JJ NN TO NNP IN DT NN VBN IN DT JJ...</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>-0.083333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The shipping off of American pork to China dur...   \n",
       "\n",
       "                                           clean_txt  \\\n",
       "0  shipping American pork China pandemic cause Ch...   \n",
       "\n",
       "                                            PoS_tags  pos_sent  neg_sent  \\\n",
       "0  DT NN IN IN JJ NN TO NNP IN DT NN VBN IN DT JJ...  0.055556  0.138889   \n",
       "\n",
       "   net_sent  \n",
       "0 -0.083333  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_demo = articles_demo[['clean_txt', 'PoS_tags', 'text', 'net_sent']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to use ColumnTransformer() to do different things to 'text', 'clean_txt', and 'PoS_tags' columns;\n",
    "# need to use FeatureUnion() to simultaneously perform dimensionality reduction and feature selection\n",
    "# on the 'clean_txt' column\n",
    "\n",
    "# Set up feature union to do both dimensionality reduction (compare NMF & LSA) and select\n",
    "# K best performing word vectors; in other words, get topics & keep important words.\n",
    "\n",
    "# set number of topics\n",
    "topics = 100\n",
    "\n",
    "# set number of words from BoW to keep in model\n",
    "top_terms = 20\n",
    "\n",
    "nmf = NMF(n_components = topics, random_state = 42)\n",
    "kbest = SelectKBest(k = top_terms)\n",
    "\n",
    "featUn = FeatureUnion([\n",
    "    ('dim_red', nmf),\n",
    "    ('feat_sel', kbest)\n",
    "])\n",
    "\n",
    "# Set up pipeline for clean text: vectorize followed by simultaneous dimensionality\n",
    "# reduction and feature selection.\n",
    "tfidf = TfidfVectorizer(stop_words = 'english',\n",
    "                        ngram_range = (1,2),\n",
    "                        max_df = 0.90,\n",
    "                        min_df = 2,\n",
    "                        max_features = 20000)\n",
    "\n",
    "cleanTextPipe = Pipeline([\n",
    "    ('vectorize', tfidf),\n",
    "    ('dimRed_plus_featSel', featUn)\n",
    "])\n",
    "\n",
    "# Set up column transformer to distinguish between vectorizing clean text and original text.\n",
    "#\n",
    "# For clean text, perform the following: TF-IDF vectorize; then perform dimensionality\n",
    "# reduction (NMF) & feature selection (SelectKBest).\n",
    "#\n",
    "# For PoS text, perform the following: vectorize PoS tags (CountVectorizer).\n",
    "countvec = CountVectorizer(lowercase = False)\n",
    "#\n",
    "# For original text, perform the following: vectorize select punctuation (CountVectorizer).\n",
    "puncvec = Punc_Vec()\n",
    "\n",
    "sep_cols = ColumnTransformer(\n",
    "    # transformers\n",
    "    [\n",
    "        ('vectorize_punctuation', puncvec, 'text'), # perform only on text column\n",
    "        ('clean_text_pipe', cleanTextPipe, 'clean_txt'), # perform only on clean_txt column\n",
    "        ('vectorize_PoS', countvec, 'PoS_tags') # perform only on PoS_tags column\n",
    "    ],\n",
    "    # keep the 'net_sent' column with no transformations\n",
    "    remainder = 'passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained column transformer\n",
    "fake_news_transformer = joblib.load('column_transformer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataframe into the feature matrix for prediction\n",
    "X = fake_news_transformer.transform(articles_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "fake_news_classifier = joblib.load('binaryEstimator_fakeNews.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.576"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict probability \"fake news\"\n",
    "fake_news_classifier.predict_proba(X)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict class\n",
    "fake_news_classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
